# Local Sub-Agents

**Purpose:** Delegate small tasks to local LLMs running via Ollama + custom agentic wrapper.

## Latest Benchmark (2026-01-27)

| Model | Tests Passed | Score | Status |
|-------|--------------|-------|--------|
| **qwen3:8b** | **10/14** | **73/100** | ü•á Best |
| gpt-oss:20b | 7/8* | 70/100 | Good |
| devstral-small-2 | 8/8* | 69/100 | Good |
| ministral-3 | 6/8* | 79/100 | Decent |

*Earlier benchmark with 8 tests. Full 14-test suite pending.

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Main Agent (Claude)                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  1. Receives task from user                                 ‚îÇ
‚îÇ  2. Decides if delegable (small, well-defined)              ‚îÇ
‚îÇ  3. Crafts detailed prompt for sub-agent                    ‚îÇ
‚îÇ  4. Spawns: ollama-agent.sh "prompt" /workdir               ‚îÇ
‚îÇ  5. Monitors completion                                     ‚îÇ
‚îÇ  6. Reviews work                                            ‚îÇ
‚îÇ  7. Reports back to user                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Local Sub-Agent                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Provider: Ollama (qwen3:8b)                                ‚îÇ
‚îÇ  Harness:  ollama-agent.sh (custom wrapper)                 ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Tools available:                                           ‚îÇ
‚îÇ  - write_file: Create/overwrite files                       ‚îÇ
‚îÇ  - read_file: Read file contents                            ‚îÇ
‚îÇ  - run_command: Execute shell commands                      ‚îÇ
‚îÇ  - list_files: List directory contents                      ‚îÇ
‚îÇ  - task_complete: Signal completion with summary            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Quick Start

```bash
# Run a task
./ollama-agent.sh "Create a hello world script" /tmp

# With different model
OLLAMA_MODEL=glm-4.7-flash:latest ./ollama-agent.sh "task" /dir

# Run benchmark suite
cd tests && ./run-tests.sh qwen3:8b

# Compare models
cd tests && ./compare-results.sh
```

---

## Test Suite

**14 tests** covering basic to advanced coding tasks:

| Tests | Focus | Examples |
|-------|-------|----------|
| 1-8 | Basics | Python scripts, configs, shell, debugging |
| 9-14 | Advanced | API clients, parsers, async, SQL, CLI tools |

**Scoring weights** (accuracy > speed):
- Correctness: 50%
- Efficiency: 20%  
- Speed: 10%
- Format: 20%

See [tests/README.md](tests/README.md) for details.

---

## Hardware Constraints

**‚ö†Ô∏è Critical:** Local models share limited GPU/CPU resources.

- **One inference at a time** ‚Äî never parallel
- Queue tasks sequentially
- Wait for completion before starting next
- Parallel runs cause: memory thrashing, OOM kills, degraded output quality

---

## Available Models

| Model | Size | Speed | Tool Support | Notes |
|-------|------|-------|--------------|-------|
| `qwen3:8b` | 5.2 GB | Fast | ‚úÖ Excellent | **Recommended default** |
| `rnj-1:8b` | ~5 GB | Very fast | ‚úÖ Excellent | Best speed/quality ratio |
| `devstral-small-2` | ~8 GB | Medium | ‚úÖ Good | Passes all basic tests |
| `gpt-oss:20b` | ~13 GB | Slower | ‚úÖ Good | Larger, more capable |
| `glm-4.7-flash` | 19 GB | Slower | ‚úÖ Good | Better reasoning |
| `ministral-3` | ~3 GB | Fast | ‚ö†Ô∏è Partial | May fail complex tests |
| `llama3.1:8b` | ~5 GB | Medium | ‚ùå Limited | Tool format issues |
| `mistral:7b` | ~4 GB | Medium | ‚ùå Limited | Tool format issues |

Default: `qwen3:8b` (set via `OLLAMA_MODEL` env var)

### Environment Variables

```bash
OLLAMA_MODEL=qwen3:8b          # Model to use
OLLAMA_TIMEOUT=120             # API timeout in seconds
OLLAMA_QUIET=1                 # Suppress verbose output
OLLAMA_MAX_ITERATIONS=10       # Max tool call loops
```

---

## Task Delegation Criteria

### ‚úÖ Good for Local Sub-Agent
- Single-file scripts
- Config file generation
- Simple utilities
- Data transformation
- Documentation snippets
- Test file creation

### ‚ùå Keep on Main Agent
- Multi-file architectural changes
- Security-sensitive operations
- Tasks requiring user interaction
- Complex debugging
- Anything needing clarification mid-task

---

## Tool Reference

| Tool | Parameters | Description |
|------|------------|-------------|
| `write_file` | path, content | Write content to file (creates dirs) |
| `read_file` | path | Read file contents |
| `run_command` | command | Execute shell command |
| `list_files` | path (optional) | List directory contents |
| `task_complete` | summary | Signal done, provide summary |

---

## Example Prompts

**Simple script:**
```
Create a bash script called cleanup.sh that removes all .tmp files 
from the current directory. Include a dry-run flag (-n). 
Make it executable. When done, call task_complete with a summary.
```

**Config generation:**
```
Create a nginx.conf for a reverse proxy that:
- Listens on port 80
- Proxies /api to localhost:3000
- Serves static files from /var/www/html
When done, call task_complete with a summary.
```

---

## Files

```
local-sub-agents/
‚îú‚îÄ‚îÄ ollama-agent.sh          # Main wrapper script
‚îú‚îÄ‚îÄ README.md                # This documentation
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ run-tests.sh         # Test runner (14 tests)
    ‚îú‚îÄ‚îÄ benchmark-all-models.sh  # Full model comparison
    ‚îú‚îÄ‚îÄ compare-results.sh   # Compare results (--json)
    ‚îú‚îÄ‚îÄ README.md            # Test documentation
    ‚îî‚îÄ‚îÄ results/             # Test output (gitignored)
```

---

## Troubleshooting

**Model outputs garbage:**
- Check if another inference is running (`ollama ps`)
- Wait for it to finish, try again

**Tool calls fail:**
- Ensure Ollama is running (`ollama serve`)
- Check model supports tools (`qwen3:8b` does)

**Slow responses:**
- Normal for first run (model loading)
- Subsequent runs faster while model is hot

---

## Why Not OpenCode?

OpenCode's Ollama integration has a bug where tool calls fail with "Invalid Tool" errors. 
Our custom `ollama-agent.sh` wrapper calls Ollama's native API directly, which works perfectly.
