# Benchmark Fixes - 2026-01-29

## Problems Identified

### 1. **Exec Timeout Too Short**
- **Issue:** Running benchmark via `exec` had 30-min timeout, but full benchmark needs 5+ hours (10 models × 30 min each)
- **Result:** Process got killed after 30 minutes, stopping mid-benchmark

### 2. **Verification Commands Had No Timeout**
- **Issue:** Test verification runs `eval "$verify_cmd"` without timeout (line 104 of run-tests.sh)
- **Result:** When models generate buggy code (e.g., infinite loops), verification hangs forever
- **Evidence:** test11_refactor showed "Running: test11_refactor" but never completed

### 3. **Child Processes Not Killed**
- **Issue:** When tests timeout, child processes might linger
- **Result:** Can cause resource leaks and interfere with subsequent tests

## Fixes Applied

### 1. Background Runner Script (`run-benchmark.sh`)
```bash
# New wrapper script to run benchmark in background with:
- Proper logging (results/benchmark_run_TIMESTAMP.log)
- PID tracking (.benchmark.pid)
- Status checking
- No timeout limit
```

**Usage:**
```bash
./run-benchmark.sh [--quick]  # Starts in background
tail -f results/benchmark_run_*.log  # Monitor progress
```

### 2. Verification Timeout Protection (`run-tests.sh`)
```bash
# Before (line 104):
if eval "$verify_cmd" > /dev/null 2>&1; then

# After:
if timeout 30 bash -c "$verify_cmd" > /dev/null 2>&1; then
```

**Impact:** Verification now has 30-second hard limit, preventing hangs from buggy generated code

### 3. Enhanced Process Cleanup (`run-tests.sh`)
```bash
# Added kill-after to ensure cleanup:
timeout --kill-after=10s "$timeout_secs" bash -c "..." || true

# Kill any lingering child processes:
pkill -P $$ -f "ollama-agent.sh" 2>/dev/null || true
```

**Impact:** Ensures no zombie processes when tests timeout

## Testing Plan

### Quick Validation (2 models, ~30 min)
```bash
cd ~/Desktop/local-sub-agents/tests
./run-benchmark.sh --quick 2>&1 | head -20  # Start it
tail -f results/benchmark_run_*.log         # Watch it
```

### Full Benchmark (10 models, 5+ hours)
```bash
./run-benchmark.sh                          # Start in background
# Walk away, check back later
cat results/BENCHMARK_SUMMARY_*.md          # View results
```

## What Should Work Now

✅ **End-to-end completion:** Benchmark will run all 10 models without manual intervention  
✅ **No hangs:** Verification timeouts prevent infinite loops from stalling tests  
✅ **Clean execution:** Proper cleanup between models  
✅ **Background mode:** Can run for hours without babysitting  
✅ **Full logging:** Everything captured in timestamped log files  

## Expected Behavior

Each model run will:
1. Verify no models loaded (cleanup check)
2. Load and warm up the model
3. Run 14 tests (each with 120-240s timeout)
4. Mark timeouts as FAIL (not crash)
5. Generate result markdown
6. Explicitly unload model
7. Wait for memory to clear
8. Move to next model

**Tests that may fail/timeout:**
- test9_api_client (complex HTTP logic)
- test10_parser (complex parsing)
- test11_refactor (complex OOP refactoring)
- test12_async (asyncio edge cases)
- test13_sql (database operations)
- test14_cli (argparse CLI)

**This is expected** - not all models can handle these complex tasks. The goal is that the benchmark **completes** and produces scores for all models.

## How to Interpret Results

- **Score 80+:** Excellent - can handle complex tasks
- **Score 60-79:** Good - solid basic capabilities
- **Score 40-59:** Moderate - struggles with complexity
- **Score <40:** Weak - basic tasks only

Pass rate and iterations matter more than raw speed for code quality.
